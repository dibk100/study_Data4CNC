{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 13698,
     "status": "ok",
     "timestamp": 1746534164593,
     "user": {
      "displayName": "Junhyuk Choi",
      "userId": "03691909873065301296"
     },
     "user_tz": -540
    },
    "id": "CnqcFIXH7BB_"
   },
   "outputs": [],
   "source": [
    "# ✅ [1] 필수 패키지 임포트\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "from scipy.stats import wasserstein_distance, energy_distance\n",
    "from scipy.spatial import distance\n",
    "from sklearn import metrics\n",
    "\n",
    "from dtaidistance import dtw  # pip install dtaidistance\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1746534164596,
     "user": {
      "displayName": "Junhyuk Choi",
      "userId": "03691909873065301296"
     },
     "user_tz": -540
    },
    "id": "ax18mb4JdKhv"
   },
   "outputs": [],
   "source": [
    "def dummy_creation(dataset, dummy_categories):\n",
    "    for i in dummy_categories:\n",
    "        dataset_dummy = pd.get_dummies(dataset[i])\n",
    "\n",
    "        dataset=pd.concat([dataset,dataset_dummy],\n",
    "                          axis=1)\n",
    "        dataset=dataset.drop(i,axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nJn-Iy568ax"
   },
   "source": [
    "## 1. 데이터 불러오기 (Load a Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh10SrP5d8Up"
   },
   "source": [
    "Data Source: https://www.kaggle.com/datasets/shasun/tool-wear-detection-in-cnc-mill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1453,
     "status": "ok",
     "timestamp": 1746534166050,
     "user": {
      "displayName": "Junhyuk Choi",
      "userId": "03691909873065301296"
     },
     "user_tz": -540
    },
    "id": "I8Tj8rY5gUAe",
    "outputId": "0057ff2e-11e8-4f99-d66a-54414e5073c3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 데이터셋이 저장된 Google Drive 내 폴더 경로 지정\n",
    "FOLDER_DIR = \"./dataset-SMART\"\n",
    "\n",
    "# 개별 실험 데이터를 저장할 리스트 초기화\n",
    "experiments = []\n",
    "\n",
    "# 폴더 내 파일들을 순회\n",
    "for file_name in os.listdir(FOLDER_DIR):\n",
    "  if file_name.startswith(\"experiment\"):\n",
    "    df = pd.DataFrame()  # 빈 데이터프레임 생성 (선택사항)\n",
    "\n",
    "    df = pd.read_csv(f\"{FOLDER_DIR}/{file_name}\", index_col=None, header=0)\n",
    "\n",
    "    # 파일명에서 실험 번호 추출하여 'Experiment' 컬럼에 추가 (예: experiment02.csv → 2)\n",
    "    df['Experiment'] = int(file_name[-6:-4])\n",
    "    experiments.append(df)\n",
    "\n",
    "# 개별 실험 데이터를 하나의 데이터프레임으로 병합\n",
    "df_raw_original = pd.concat(experiments, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746534167872,
     "user": {
      "displayName": "Junhyuk Choi",
      "userId": "03691909873065301296"
     },
     "user_tz": -540
    },
    "id": "-Da4QwCkdVZk"
   },
   "outputs": [],
   "source": [
    "# 메타데이터 CSV 파일 읽기 (조건 정보 포함)\n",
    "df_meta = pd.read_csv(os.path.join(FOLDER_DIR, \"train.csv\"))\n",
    "\n",
    "# tool_condition, clamp_pressure, feedrate 순으로 정렬\n",
    "df_meta = df_meta.sort_values(by=['tool_condition', 'clamp_pressure', 'feedrate'])\n",
    "\n",
    "# tool_condition이 'worn'이면 1, 아니면 0으로 라벨링 >>> 1이면 마모됨. 0이면 정상\n",
    "df_meta['label'] = df_meta['tool_condition'].apply(lambda x: 1 if x == \"worn\" else 0)\n",
    "\n",
    "# feedrate와 clamp_pressure를 조합해 WorkingCondition이라는 새로운 컬럼 생성 >>> 이동속도와 클램핑 압력열을 조합한 파생 변수 생성. ex) 3-2.5\n",
    "df_meta['WorkingCondition'] = df_meta['feedrate'].apply(lambda x: str(x)) + \"-\" + df_meta['clamp_pressure'].apply(lambda x: str(x))\n",
    "\n",
    "df_raw = dummy_creation(df_raw_original, ['Machining_Process'])\n",
    "\n",
    "df_raw['label'] = 0         ## 공구 상태가 정상\n",
    "df_raw['label'] = df_raw['Experiment'].apply(lambda x: df_meta[df_meta['No'] == x]['label'].values[0])      # 실험번호와 라벨 맞추기\n",
    "\n",
    "no_wc_dict = dict(zip(df_meta['No'], df_meta['WorkingCondition']))\n",
    "\n",
    "df_raw['WorkingCondition'] = df_raw['Experiment'].apply(lambda x: no_wc_dict[x])\n",
    "\n",
    "# 'WorkingCondition'을 '-' 기준으로 나누어 두 개의 새 컬럼 생성\n",
    "df_raw[['feedrate', 'clamp_pressure']] = df_raw['WorkingCondition'].str.split('-', expand=True)\n",
    "\n",
    "# 필요한 경우 형변환 (예: 정수 또는 실수형으로)\n",
    "df_raw['feedrate'] = df_raw['feedrate'].astype(int)\n",
    "df_raw['clamp_pressure'] = df_raw['clamp_pressure'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1746534625678,
     "user": {
      "displayName": "Junhyuk Choi",
      "userId": "03691909873065301296"
     },
     "user_tz": -540
    },
    "id": "s3VQrWUoTZqO"
   },
   "outputs": [],
   "source": [
    "col_sensor = ['X1_ActualPosition', 'X1_ActualVelocity', 'X1_ActualAcceleration',\n",
    "       'X1_CommandPosition', 'X1_CommandVelocity', 'X1_CommandAcceleration',\n",
    "       'X1_CurrentFeedback', 'X1_DCBusVoltage', 'X1_OutputCurrent',\n",
    "       'X1_OutputVoltage', 'X1_OutputPower', 'Y1_ActualPosition',\n",
    "       'Y1_ActualVelocity', 'Y1_ActualAcceleration', 'Y1_CommandPosition',\n",
    "       'Y1_CommandVelocity', 'Y1_CommandAcceleration', 'Y1_CurrentFeedback',\n",
    "       'Y1_DCBusVoltage', 'Y1_OutputCurrent', 'Y1_OutputVoltage',\n",
    "       'Y1_OutputPower', 'Z1_ActualPosition', 'Z1_ActualVelocity',\n",
    "       'Z1_ActualAcceleration', 'Z1_CommandPosition', 'Z1_CommandVelocity',\n",
    "       'Z1_CommandAcceleration', 'Z1_CurrentFeedback', 'Z1_DCBusVoltage',\n",
    "       'Z1_OutputCurrent', 'Z1_OutputVoltage', 'S1_ActualPosition',\n",
    "       'S1_ActualVelocity', 'S1_ActualAcceleration', 'S1_CommandPosition',\n",
    "       'S1_CommandVelocity', 'S1_CommandAcceleration', 'S1_CurrentFeedback',\n",
    "       'S1_DCBusVoltage', 'S1_OutputCurrent', 'S1_OutputVoltage',\n",
    "       'S1_OutputPower', 'S1_SystemInertia']\n",
    "\n",
    "col_bn = ['X1_ActualPosition', 'X1_ActualVelocity', 'X1_ActualAcceleration',\n",
    "       'X1_CommandPosition', 'X1_CommandVelocity', 'X1_CommandAcceleration',\n",
    "       'X1_CurrentFeedback', 'X1_DCBusVoltage', 'X1_OutputCurrent',\n",
    "       'X1_OutputVoltage', 'X1_OutputPower', 'Y1_ActualPosition',\n",
    "       'Y1_ActualVelocity', 'Y1_ActualAcceleration', 'Y1_CommandPosition',\n",
    "       'Y1_CommandVelocity', 'Y1_CommandAcceleration', 'Y1_CurrentFeedback',\n",
    "       'Y1_DCBusVoltage', 'Y1_OutputCurrent', 'Y1_OutputVoltage',\n",
    "       'Y1_OutputPower', 'Z1_ActualPosition', 'Z1_ActualVelocity',\n",
    "       'Z1_ActualAcceleration', 'Z1_CommandPosition', 'Z1_CommandVelocity',\n",
    "       'Z1_CommandAcceleration', 'Z1_CurrentFeedback', 'Z1_DCBusVoltage',\n",
    "       'Z1_OutputCurrent', 'Z1_OutputVoltage', 'S1_ActualPosition',\n",
    "       'S1_ActualVelocity', 'S1_ActualAcceleration', 'S1_CommandPosition',\n",
    "       'S1_CommandVelocity', 'S1_CommandAcceleration', 'S1_CurrentFeedback',\n",
    "       'S1_DCBusVoltage', 'S1_OutputCurrent', 'S1_OutputVoltage',\n",
    "       'S1_OutputPower', 'S1_SystemInertia','feedrate','clamp_pressure','layer']\n",
    "\n",
    "col_CNCCode = ['M1_CURRENT_PROGRAM_NUMBER',\n",
    "       'M1_sequence_number', 'M1_CURRENT_FEEDRATE', 'Experiment', 'End',\n",
    "       'Layer 1 Down', 'Layer 1 Up', 'Layer 2 Down', 'Layer 2 Up',\n",
    "       'Layer 3 Down', 'Layer 3 Up', 'Prep', 'Repositioning', 'Starting',\n",
    "       'end', 'label']\n",
    "\n",
    "# 절삭 구간만 자르기 -> 공구 마모와 관련된 구간\n",
    "layer_cols = ['Layer 1 Down', 'Layer 1 Up', 'Layer 2 Down', 'Layer 2 Up', 'Layer 3 Down', 'Layer 3 Up']\n",
    "df_raw['Layer_Total'] = df_raw[layer_cols].sum(axis=1)  # axis=1로 행 방향 합계\n",
    "df_layer = df_raw[df_raw['Layer_Total'] > 0].copy() # .copy()를 사용하여 복사본 생성\n",
    "\n",
    "# Layer to Working Conditions\n",
    "df_layer.loc[:, 'Layer1'] = df_layer[['Layer 1 Down', 'Layer 1 Up']].sum(axis=1)\n",
    "df_layer.loc[:, 'Layer2'] = df_layer[['Layer 2 Down', 'Layer 2 Up']].sum(axis=1)\n",
    "df_layer.loc[:, 'Layer3'] = df_layer[['Layer 3 Down', 'Layer 3 Up']].sum(axis=1)\n",
    "\n",
    "df_layer.loc[:, 'Layer_Info'] = df_layer[['Layer1', 'Layer2', 'Layer3']].idxmax(axis=1)  # idxmax로 최대값 컬럼명 가져오기\n",
    "df_layer.loc[:, 'Layer_Info'] = df_layer['Layer_Info'].map({'Layer1': 1, 'Layer2': 2, 'Layer3': 3})  # 컬럼명을 숫자로 매핑\n",
    "\n",
    "# Scenario = Experiment (Load X Speed X Label) X Layer\n",
    "df_layer.loc[:, 'SCN'] = df_layer['Experiment'].astype(str) + \"-\" + df_layer['Layer_Info'].astype(str)  # 문자열로 변환하여 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고장 진단 모델 성능 지표 개발"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semi-supervised-learning, base 지표로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training with 10% labeled data ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      8042\n",
      "           1       0.99      1.00      0.99      9478\n",
      "\n",
      "    accuracy                           0.99     17520\n",
      "   macro avg       0.99      0.99      0.99     17520\n",
      "weighted avg       0.99      0.99      0.99     17520\n",
      "\n",
      "\n",
      "=== Training with 50% labeled data ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      8042\n",
      "           1       1.00      1.00      1.00      9478\n",
      "\n",
      "    accuracy                           1.00     17520\n",
      "   macro avg       1.00      1.00      1.00     17520\n",
      "weighted avg       1.00      1.00      1.00     17520\n",
      "\n",
      "\n",
      "=== Training with 100% labeled data ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      8042\n",
      "           1       1.00      1.00      1.00      9478\n",
      "\n",
      "    accuracy                           1.00     17520\n",
      "   macro avg       1.00      1.00      1.00     17520\n",
      "weighted avg       1.00      1.00      1.00     17520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# X, y 정의\n",
    "X = df_layer.drop(columns=['label', 'SCN','WorkingCondition'])  # 필요 없는 컬럼 제거\n",
    "y = df_layer['label']\n",
    "\n",
    "# 비율별 split (10%, 50%, 100%)\n",
    "for ratio in [0.1, 0.5, 1.0]:\n",
    "    print(f\"\\n=== Training with {int(ratio*100)}% labeled data ===\")\n",
    "\n",
    "    if ratio < 1.0:\n",
    "        X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(\n",
    "            X, y, train_size=ratio, stratify=y, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        X_labeled, y_labeled = X.copy(), y.copy()\n",
    "        X_unlabeled, y_unlabeled = pd.DataFrame(columns=X.columns), pd.Series(dtype=y.dtype)\n",
    "\n",
    "    # 초기 모델 학습\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_labeled, y_labeled)\n",
    "\n",
    "    # Pseudo-label 예측\n",
    "    if not X_unlabeled.empty:\n",
    "        pseudo_labels = clf.predict(X_unlabeled)\n",
    "\n",
    "        # Labeled + pseudo-labeled 데이터로 재학습\n",
    "        X_combined = pd.concat([X_labeled, X_unlabeled])\n",
    "        y_combined = pd.concat([y_labeled, pd.Series(pseudo_labels, index=X_unlabeled.index)])\n",
    "    else:\n",
    "        X_combined = X_labeled\n",
    "        y_combined = y_labeled\n",
    "\n",
    "    final_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    final_model.fit(X_combined, y_combined)\n",
    "\n",
    "    # 전체 평가\n",
    "    y_pred = final_model.predict(X)\n",
    "    print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training with 10% labeled data ===\n",
      "TP=27, FP=47, FN=0, Custom Score=3.500\n",
      "\n",
      "=== Training with 50% labeled data ===\n",
      "TP=27, FP=6, FN=0, Custom Score=24.000\n",
      "\n",
      "=== Training with 100% labeled data ===\n",
      "TP=27, FP=6, FN=0, Custom Score=24.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === 고장 구간 탐지를 위한 유틸리티 ===\n",
    "def get_event_ranges(y, min_length=1, target_value=1):\n",
    "    \"\"\"지정된 target_value의 연속된 구간(start, end index) 반환\"\"\"\n",
    "    events = []\n",
    "    in_event = False\n",
    "    start = 0\n",
    "    for i, val in enumerate(y):\n",
    "        if val == target_value and not in_event:\n",
    "            in_event = True\n",
    "            start = i\n",
    "        elif val != target_value and in_event:\n",
    "            end = i\n",
    "            if end - start >= min_length:\n",
    "                events.append((start, end))\n",
    "            in_event = False\n",
    "    if in_event:\n",
    "        end = len(y)\n",
    "        if end - start >= min_length:\n",
    "            events.append((start, end))\n",
    "    return events\n",
    "\n",
    "def temporal_metric(y_true, y_pred, tolerance=0, w_tp=1.0, w_fp=1.0, w_fn=1.0):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    true_events = get_event_ranges(y_true)\n",
    "    pred_events = get_event_ranges(y_pred)\n",
    "\n",
    "    TP = 0\n",
    "    matched_pred = set()\n",
    "\n",
    "    for t_start, t_end in true_events:\n",
    "        matched = False\n",
    "        for i, (p_start, p_end) in enumerate(pred_events):\n",
    "            if p_end >= t_start - tolerance and p_start <= t_end + tolerance:\n",
    "                if i not in matched_pred:\n",
    "                    matched_pred.add(i)\n",
    "                    matched = True\n",
    "                    break\n",
    "        if matched:\n",
    "            TP += 1\n",
    "\n",
    "    FN = len(true_events) - TP\n",
    "    FP = len(pred_events) - len(matched_pred)\n",
    "\n",
    "    custom_score = (w_tp * TP) - (w_fp * FP) - (w_fn * FN)\n",
    "    \n",
    "    return {\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'custom_score': custom_score\n",
    "    }\n",
    "\n",
    "def temporal_metric_by_scn(y_true, y_pred, scn, tolerance=0, w_tp=1.0, w_fp=0.5, w_fn=2.0):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: np.array 또는 list 형태의 전체 라벨 시퀀스\n",
    "    scn: 각 인덱스에 대응하는 SCN 정보 (list or np.array)\n",
    "    tolerance, w_tp, w_fp, w_fn: 기존과 동일\n",
    "    \n",
    "    SCN별로 분리하여 temporal_metric 계산 후 합산 반환\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    scn = np.array(scn)\n",
    "\n",
    "    unique_scn = np.unique(scn)\n",
    "    total_TP, total_FP, total_FN = 0, 0, 0\n",
    "\n",
    "    for s in unique_scn:\n",
    "        idx = (scn == s)\n",
    "        y_true_s = y_true[idx]\n",
    "        y_pred_s = y_pred[idx]\n",
    "\n",
    "        # 기존 temporal_metric 함수 재활용\n",
    "        metrics = temporal_metric(y_true_s, y_pred_s, tolerance, w_tp, w_fp, w_fn)\n",
    "        \n",
    "        total_TP += metrics['TP']\n",
    "        total_FP += metrics['FP']\n",
    "        total_FN += metrics['FN']\n",
    "\n",
    "    custom_score = (w_tp * total_TP) - (w_fp * total_FP) - (w_fn * total_FN)\n",
    "\n",
    "    return {\n",
    "        'TP': total_TP,\n",
    "        'FP': total_FP,\n",
    "        'FN': total_FN,\n",
    "        'custom_score': custom_score\n",
    "    }\n",
    "\n",
    "\n",
    "# === 모델 훈련 및 평가 ===\n",
    "X = df_layer.drop(columns=['label', 'SCN','WorkingCondition'])  # 필요 없는 컬럼 제거\n",
    "y = df_layer['label']\n",
    "\n",
    "for ratio in [0.1, 0.5, 1.0]:\n",
    "    print(f\"\\n=== Training with {int(ratio*100)}% labeled data ===\")\n",
    "\n",
    "    if ratio < 1.0:\n",
    "        X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(\n",
    "            X, y, train_size=ratio, stratify=y, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        X_labeled, y_labeled = X.copy(), y.copy()\n",
    "        X_unlabeled, y_unlabeled = pd.DataFrame(columns=X.columns), pd.Series(dtype=y.dtype)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_labeled, y_labeled)\n",
    "\n",
    "    if not X_unlabeled.empty:\n",
    "        pseudo_labels = clf.predict(X_unlabeled)\n",
    "        X_combined = pd.concat([X_labeled, X_unlabeled])\n",
    "        y_combined = pd.concat([y_labeled, pd.Series(pseudo_labels, index=X_unlabeled.index)])\n",
    "        clf.fit(X_combined, y_combined) \n",
    "\n",
    "    final_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    final_model.fit(X_combined, y_combined)\n",
    "\n",
    "    # 예측 및 커스텀 지표 계산\n",
    "    y_pred = final_model.predict(X)\n",
    "\n",
    "    metrics = temporal_metric_by_scn(\n",
    "        y_true=df_layer['label'],\n",
    "        y_pred=y_pred,\n",
    "        scn=df_layer['SCN'],\n",
    "        tolerance=3,\n",
    "        w_tp=1.0,\n",
    "        w_fp=0.5,\n",
    "        w_fn=2.0\n",
    "    )\n",
    "\n",
    "    print(f\"TP={metrics['TP']}, FP={metrics['FP']}, FN={metrics['FN']}, Custom Score={metrics['custom_score']:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyORvbEzgfnOPIk/tK+XjdlM",
   "collapsed_sections": [
    "smmyhQRB5vxA",
    "LFFPwq6JfkVb",
    "QrV1hPsvftHe"
   ],
   "name": "",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "diwork_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
